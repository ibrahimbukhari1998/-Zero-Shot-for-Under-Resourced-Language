# Project Goal
**Compare the performance of XLM-R and Glot500** when fine-tuned for POS tagging on a better-resourced language and then applied directly to a low-resource language without further training. Analyze the impact of subword tokenization on cross-lingual transfer.

## Key Points:
- **Well-defined task**
- **Low-resourced languages**: To be determined (TBD)
- **Contact**: [dbernhard@unistra.fr](mailto:dbernhard@unistra.fr)

## Steps for the Project:
1. **Model Fine-Tuning**  
   - Use datasets from better-resourced languages within the Universal Dependencies framework.
   
2. **Zero-Shot Transfer**  
   - Apply the model to low-resource languages with existing POS annotated corpora (for evaluation purposes).
   
3. **Subword Tokenization Analysis**  
   - Investigate how differences in tokenization between source and target languages impact the performance of zero-shot POS tagging.
